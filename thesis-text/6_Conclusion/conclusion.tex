\chapter{Conclusion and Further work}
\label{chapter:Conclusion_and_further_work}
\thispagestyle{myheadings}

% set this to the location of the figures for this chapter. it may
% also want to be ../Figures/2_Body/ or something. make sure that
% it has a trailing directory separator (i.e., '/')!
\graphicspath{}

\section{Conclusion}
The goal of this thesis is to act as a proof of concept for the application of Deep Reinforcement Learning for optimization of query processing on streams.\\
Say, for a particular window of data 
\begin{itemize}
    \item The size of SegVol $=x$.
    \item The size of SegAvgSpeed $=y$.
\end{itemize}
Due to the query used in the experiments, the number of operations required to execute the query lie between 
$$[x*y,4*x*y]$$ 
The worst and best possible cases can both arise in the same window depending on the order used and the data in the query.\\
Of the $24$ possible operation orders, we were able to predict the optimal order of operations around $48\%$ cases, reducing the number of operations to around $39\%$ of the worst observed(which is better than the actual worst).
\begin{center}
$x*y \leq$ optimal $\leq$ predicted $\leq$ worst observed $\leq 4*x*y$
\end{center}

\subsection{Take aways}
We hope the material in this thesis is sufficient for anyone to understand the complexities of SQL and be able to work on their own versions and other improvements, overall we hope the thesis provided the read with the following:-
\begin{itemize}
    \item The read should now have to tools to understand inner workings of SQL
    \item Be familiar with Operator Algebra.
    \item Understand how queries are executed in SQL.
    \item Understand how physical plans are structed/ constructed in SQL.
    \item Get familiar with what data streams are and what they are used for.
    \item Understand what deep learning, and reinforcement learning is.
    \item And see how DQN are being applied for improving operation ordering. 
\end{itemize}
\section{Further Work}
This thesis proposed a method for optimization of query execution on data stream and provided a demo implementation for experimentation. There are still many areas which can be improved and developed further.

\subsection{Data Generation}
As shown in chapter $5$, the data generated is highly biased, there are ways to try to tackle it\cite{Losing2018IncrementalOL}. Imbalanced datasets are characterized by one class outnum-bering the instances of the other one. The later is referred to as the minority class, while the former is identified as  the  majority class. These concepts can be generalized to  multi class classification  and  other  learning  tasks,  e.g.,regression.  The  imbalance  may  be  inherent  to  the problem (intrinsic) or caused by some fault in the data acquisition (extrinsic).  Learning from imbalanced datasets is challenging as most learning algorithms are designed to optimize for generalization, and as a consequence, the minority class may be completely ignored.\cite{DNN_for_stream} 
\par A source producing more diverse data and a query on that may lead to more interesting results. Real world data stream can be used to get better variety in the data and take input via different methods.
\par Some examples of data generators are :-
\begin{itemize}
    \item A real-estate website tracks a subset of data from consumersâ€™ mobile devices and makes real-time property recommendations of properties to visit based on their geo-location.\cite{data_source}
    \item A solar power company has to maintain power throughput for its customers, or pay penalties. It implemented a streaming data application that monitors of all of panels in the field, and schedules service in real time, thereby minimizing the periods of low throughput from each panel and the associated penalty payouts. \cite{data_source}
    \item A media publisher streams billions of clickstream records from its online properties, aggregates and enriches the data with demographic information about users, and optimizes content placement on its site, delivering relevancy and better experience to its audience.\cite{data_source}
    \item An online gaming company collects streaming data about player-game interactions, and feeds the data into its gaming platform. It then analyzes the data in real-time, offers incentives and dynamic experiences to engage its players.\cite{data_source}
\end{itemize}

\subsection{Data Storage}
As demonstrated in the implementation chapter, currently for executing the query, we store data in vectors. SQL systems use B-Trees which are cache optimized, this can reduce the runtime by huge margin. The choice of data storage, method of access and the target used for reinforcement learning can lead to a variety of learnt models.
\par Few considerations that can lead to changes are:-
\begin{itemize}
    \item Use of B-trees to improve runtime and access time.
    \item Use of filesystems such as AWS S3 bucket, Google Cloud Platform can affect memory access time.
    \item In cloud storage, the amount of traffic can often lead to delays, so the network setup is also a factor to consider.
    \item An experiment time requirements for query execution with various types of filesystems(HDFS, NTFS, EXT2, EXT3, EXT4, XFS, ZFS, BTRFS) can be done. 
\end{itemize} 

\subsection{Features extraction}
The idea behind the feature extraction was to have a fixed sized vector for the input to the neural network. Entropy is a measure of information and using the entropy itself in the neural network results in a loss of information. So different feature extractions can lead to better results. One method that can be used is:-
\begin{itemize}
    \item For each column, we replace the entropy with a vector of $i+1$ numbers, which represents the normalized frequency of the $i$ most frequent elements and the last one is the entropy.
\end{itemize}
This method will allow for more information about the data to be passed on to the neural network and to get better training results.


\subsection{Deep Reinforcement Learning}
The Deep Reinforcement Learning model used was simplified, in the sense that it was a $1$ move game. We start with a given state and can execute any of the $24$ possible operations orders and we end the game in the final state and report the time of execution and number of operations required.
\par A better reinforcement learning model as created here\cite{drl_dbms} can be used, it represent the transition, the state changes and more information is passed along and helps train a better model. It is important to note the initial training time and memory requirement are not system breaking, it is the online learning part which needs to be efficient

\subsection{Integration}
As mentioned, the code implemented in the thesis is meant to serve as a proof of concept. The code presented mimics the execution of a SQL query on a very high level. A system can be developed which:-
\begin{itemize}
    \item Parses text in the for of SQL query and convert it into actual SQL query.
    \item Applied various optimizations used in a SQL server.
    \item Additionally give the option to use custom optimizer at various levels to test of various implementation.
    \item Helps scalability, improves reliability and resilience.
    \item Gives options to input various evaluation metrics.
\end{itemize}

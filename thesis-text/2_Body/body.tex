\chapter{Body of my thesis}
\label{chapter:body}
\thispagestyle{myheadings}

% set this to the location of the figures for this chapter. it may
% also want to be ../Figures/2_Body/ or something. make sure that
% it has a trailing directory separator (i.e., '/')!
\graphicspath{{2_Body/Figures/}}
\section{SQL Query compiler}
\label{sec:sql}
The steps involved are
\subsection{Parsing}
In a very general sense, given an SQL query, SQL converts it into a parse tree based on SQL grammar.
\subsection{Preprocessing}
This step has several functions.
\par If a "view" is used in the query as a relation, then each instance has to be replaced by the parse tree.
\par The preprocessor also has to conduct semantic checking, that is, check if relations used exist, check for ambiguity, and type checking.
\section{System R}
\label{sec:systemR}
\section{Deep Reinforcement learning}
\label{sec:drl}

Markov decision process(MDP) is used to formalize various types of stochastic processes. In MDPs, the goal of the agent is to make a sequence of actions to optimize/ maximize an objective function. \\
Formally a MDP is a $5-$tuple 
\begin{center}
$\langle S,A,P(s,a),R(s,a), s_0 \rangle $\\
$S \to $ Set of all possible states the agent can be in.\\
$A \to $ Set of all possible actions the agent can take.\\
$P(s,a) \to $ A probability distribution of going to various states given current state and action. $s^{1} \sim P(s,a)$\\
$R(s,a) \to $ Reward for taking action $a$ on state $s$.\\
$s_0 \to $ Describes the initial state of the system/ agent.
\end{center}
The performance of the agent is measured using the rewards collected along the way through various states. So the objective of an MDP is to find a policy $\pi:S\rightarrow A$, a function that maps states to actions, in order to maximize the expected value:-
\begin{center}
    \[ 
    \argmax_{\pi} \mathbb{E} \left[ \sum^{T-1}_{t=0}R(s_{t},a_{t})  \right]
    \] 
    subject to $s_{t+1} = P(s_{t},a_{t}), a_{t} = \pi{s_{t}}$
\end{center}
This method does not reduce the search space, and unlikely greedy solution, this will lead to an optimal solution. This method does not reduce the search space, and unlikely greedy solution, this will lead to an optimal solution.
\par Reinforcement learning(RL) is a technique which optimizes MDPs iteratively, by running a simulation in each iteration and changing the policy to find an optimal one based on the cumulative reward.
\section{Relations}
\label{sec:relations}
A common method/ data structure used to formalize joins
\par \textbf{Query Graph $\to$} A query graph $G$ is an undirected graph, where each relation R is a vertex and each join predicate $\rho$ defines an edge between $2$ vertices. Let $\kappa_G$ denote the number of connected components in $G$
\par A join of relation $R_1, R_2$, in the graph corresponds to remove the vertices $v_{R_1}, v_{R_2}$, replacing them with a vertex $v_{R_1+R_2}$, the edges of the form $(v_{R_1},v) \& (v_{R_2},v)$ are replaced by $(v_{R_1+R_2}, v)$. Note each reduction reduces number of vertices by one, so this process is repeated until there are $\kappa_G$ number of vertices left.
\par \textbf{Join Optimization Problem $\to$} Let $G$ be a query graph and $J$ be a join cost model. Find sequence, $c_1\circ c_2\circ ... \circ c_n$ resulting in $|V| = \kappa_G$ to minimize
\begin{center}
    \[
        \min_{c_1,c_2,...c_n} \sum^{n}_{i=1}J(c_i)
    \] 
    subject to $G_{i+1} = c(G_i)$
\end{center}
Using these definitions, we define a MDP. 
\begin{center}
$\langle \{G_0,G_1,...G_T\},c,P(G,c),-J, G \rangle $\\
\end{center}
We are still not certain about how the cost function $J$ is structured. We are still not certain about how the cost function $J$ is structured. We are still not certain about how the cost function $J$ is structured.

We are still not certain about how the cost function $J$ is structured.










\chapter{Implementation}
\label{chapter:implementation}
\thispagestyle{myheadings}

% set this to the location of the figures for this chapter. it may
% also want to be ../Figures/2_Body/ or something. make sure that
% it has a trailing directory separator (i.e., '/')!
\graphicspath{}
This chapter explores the setup required to conduct the experiments. The chapter is divided into 
\begin{itemize}
\item Data Generation:- We generate the data stream for the Linear road benchmark.
\item Query Execution:- We have written a C++ code to execute the query and extract information while executing.
\item Deep Reinforcement Learning:- Written a Deep reinforcement learning agent to help optimize the order of operations.
\end{itemize}

\section{Data Generation}
The data is generated using the walmart linear road code \cite{walmart_linearoad} without mitsim.\\

\begin{lstlisting}[language=bash]
java com.walmart.linearroad.generator.LinearGen [-o <output file>] [-x <number of xways>] [-m <dummy value to activate multi-threading>]
\end{lstlisting}

\subsection{Schema}
The above generated data can be interpretated as follows:-
\begin{center}
\begin{tabular}{ |c|c| } 
 \hline
  \multirow{5}{4em}{Column$1$} & Tells the type of query. \\  
            &	0: position report\\
			&	2: account balance request\\
			&	3: daily expenditure request\\
			&	4: travel time request	\\
 \hline
  Column$2$ & Timestamp position. \\  
 \hline
  Column$3$ & Vehicle identification number\\  
 \hline
  Column$4$ & Speed of the vehicle \\  
 \hline
  Column$5$ & Express way number \\  
 \hline
  Column$6$ & Lane ID $(0,...,4)$\\  
 \hline
  Column$7$ & Direction of movement($0=$East or $1=$West) \\  
 \hline
  Column$8$ & Segment ID $(0,...,99)$ \\
 \hline
  Column$9$ & Position of the vehicle. \\  
 \hline
  Column$10$ & Query identifier \\  
 \hline
  Column$11$ & Start Segment \\  
 \hline
  Column$12$ & End Segment \\  
 \hline
  Column$13$ & Day of the week \\  
 \hline
  Column$14$ & Minute of the day \\  
 \hline
  Column$15$ & Day in the past $10$ weeks \\  
 \hline
\end{tabular}
\end{center}

\section{Query Execution}
The idea was to combine different data streams into $1$, but given this data as input, it is already combined.\\
We implement $C++$ code for both the simpler query and the complex query in the previous section.
\subsection{Simpler Query}
The goal of implementing the simpler query was to get a sense of time required for executing the queries.\\
To start, a window size was fixed which determined the size/ number of entries to read and store at once, the window size was interpretted in two ways, either the number of entries to read or the time interval for which to read the entries. This value was passed to the read function everytime.
\begin{lstlisting}[language=C++]
FILE *pFile;
pFile = fopen (argv[1],"r");
int database[window_size][4];
std::vector<int> vect;
vect.clear();
std::vector<std::vector<int>> output;
auto start = std::chrono::high_resolution_clock::now();
while(!feof(pFile))
{
    input_database(pFile, database, window_size);
    format(database, output, vect, window_size);
    output.clear();
    vect.clear();
}
auto stop = std::chrono::high_resolution_clock::now(); 
auto duration = std::chrono::duration_cast<std::chrono::microseconds>(stop - start);
std::cout << duration.count() << std::endl;
fclose(pFile);
\end{lstlisting}
The input was taken by simply reading a file and storing the attribute values important to the query. 
\begin{lstlisting}[language=C++]
void input_database(FILE * pFile, int d[][4],int window_size)
{
    int i,j;
    for(i=0;i<window_size;i++)
    {
        fscanf (pFile, "%i,%i,%i,%i,%i,%i,%i,%i,%i,%i,%i,%i,%i,%i,%i", &j,
        &j,&j,&d[i][0],&d[i][1],&j,&d[i][2],&d[i][3],&j,&j,&j,&j,&j,&j,&j);
    }
}
\end{lstlisting}
The next part where the actual query is executed, to execute the aggregate functions, first insert all the data into a vector of vectors, then sort vector using an ordering determined by the group by parameters. 
\begin{lstlisting}[language=C++]
for(i;i<window_size;i++)
{
    if(d[i][2]==0)
    {
        vect.push_back(d[i][1]);
        vect.push_back(d[i][2]);
        vect.push_back(d[i][3]);
        vect.push_back(d[i][0]);
        output.push_back(vect);
    }
    vect.clear();
}
std::sort(output.begin(),output.end(), my_sort);
\end{lstlisting}
Finally do the aggregation
\begin{lstlisting}[language=C++]
int count = 1;
std::sort(output.begin(),output.end(), my_sort);

std::vector<std::vector<int>> op;
op.push_back(output[0]);
for(i=1;i<output.size();i++)
{
    if((output[i][0]==output[i-count][0])&&(output[i][1]==output[i-count][1])&&(output[i][2]==output[i-count][2]))
    {
        op.back()[3]=op.back()[3]+output[i][3];
        count++;
    }
    else
    {
        op.back()[3]=op.back()[3]/(count);
        op.push_back(output[i]);
        count = 1;
    }
}
op.back()[3]=op.back()[3]/count;
output.clear();
output=op;
op.clear();
\end{lstlisting}

\subsection{Complex Query}
For the complex query we also need to store data for our DRL model. We start by giving the general outline of the code. We first define the window size in similar way to the simple query and then loop over the Linear road data file.
\begin{lstlisting}[language=C++]
int window_size=10000;
FILE *pFile;
pFile = fopen (argv[1],"r");

std::ofstream op_file;
op_file.open("q3out.txt");

std::vector<std::vector<int>> database, curCarSeg, segAvgSpeed, segVol;

int temp;
while(!feof(pFile))
{
    
    input_database(pFile, database, window_size);
    read_times++;
    printf("read times is, %d\n", read_times);
    curcarseg(database, curCarSeg);
    temp=curCarSeg.size();
    printf("    CurCarSeg size is, %d\n", temp);
    segavgspeed(database,segAvgSpeed);
    temp=segAvgSpeed.size();
    printf("    SegAvgSpeed size is, %d\n", temp);
    segvol(curCarSeg,segVol);
    temp=segVol.size();
    printf("    SegVol size is, %d\n", temp);
    segtoll(segAvgSpeed,segVol,op_file);
    
    curCarSeg.clear();
    segAvgSpeed.clear();
    segVol.clear();
    database.clear();
}
fclose(pFile);
op_file.close();
\end{lstlisting}
In each loop we calculate the $CurCarSeg$, $SegAvgSpeed$ and $SegVol$ vectors.\\
For $CurCarSeg$, simply filter the cars for the last $5$ mins.\\
For $SegAvgSpeed$, we first filter the cars which were active in the last $5$ mins, then to apply aggregate function, initially sort them, then apply the function 
\begin{lstlisting}[language=C++]
    std::vector<std::vector<int>> temp;
    int cur_time=0;
    int i,j;
    std::vector<int> vect;
    vect.resize(4);
    for(i=0;i<d.size();i++)
    {
        if(cur_time<d[i][0])
        {
            cur_time=d[i][0];
        }
    }
    for(i=0;i<d.size();i++)
    {
        if(d[i][0]>cur_time-300)
        {
            vect[0]=d[i][2];
            vect[1]=d[i][3];
            vect[2]=d[i][4];
            vect[3]=d[i][6];
            temp.push_back(vect);
        }
    }
    //temp=()
    std::sort(temp.begin(),temp.end(), my_sort);
    //(express,dir,seg, average speed)
    int count = 1;
    s.push_back(temp[0]);
    for(i=1;i<temp.size();i++)
    {
        if(temp[i][0]==temp[i-1][0] and temp[i][1]==temp[i-1][1] and temp[i][2]==temp[i-1][2])
        {
            s.back()[3]=s.back()[3]+temp[i][3];
            count++;
        }
        else
        {
            s.back()[3]=s.back()[3]/(count);
            s.push_back(temp[i]);
            count = 1;
        }
    }
    s.back()[3]=s.back()[3]/count;
    temp.clear();
\end{lstlisting}
Next, calculate $SegVol$, first sort the vector by the attributes in the group by statement. Then apply the aggregation operation.
\begin{lstlisting}[language=C++]
//d=(carid,exp,dir,seg)
std::sort(d.begin(),d.end(), my_sort2);
int i,j;
s.push_back(d[0]);
s.back()[0]=1;
for(i=1;i<d.size();i++)
{
    if(d[i][3]==d[i-1][3] and d[i][1]==d[i-1][1] and d[i][2]==d[i-1][2])
    {
        s.back()[0]++;
    }
    else
    {
        s.push_back(d[i]);
        s.back()[0]=1;
    }
}
\end{lstlisting}
After obtaining both $SegVol$ and $SegAvgSpeed$, finally proceed to calculate $SegToll$. While calculating $SegToll$, also calculate and store the column wise entropy, the size of $SegVol$ and $SegAvgSpeed$. This is the part where we try to optimize, i.e. find the best order of operations. so we record the time taken and the number of operations required for each order of operations.
\begin{lstlisting}[language=C++]
int c1,r1,i,j,k,operation;
std::vector<float> entropy;
entropy=entropy_calc(s,v);


for(i=0;i<entropy.size();i++)
{
    op_file<<entropy[i]<<" ";
}
op_file<<std::endl;
op_file<<s.size()<<" "<<v.size()<<std::endl;
entropy.clear();
//v=(count of cars,exp,dir,seg)
//s=(express,dir,seg, average speed)
int times;
std::vector<int> order;
order.push_back(0);
order.push_back(1);
order.push_back(2);
order.push_back(3);

int max_times=24;
int num_op;
bool b;
for(times=0;times<max_times;times++)
{
    num_op=0;
    auto start = std::chrono::high_resolution_clock::now();
    for(i=0;i<s.size();i++)
    {
        for(j=0;j<v.size();j++)
        {
            b=false;
            for(k=0;k<4;k++)
            {
                operation=order[k];
                num_op++;
                switch(operation)
                {
                    case 0:
                        if(s[i][0]!=v[j][1])
                        {
                            b=true;
                        }
                    case 1:
                        if(s[i][1]!=v[j][2])
                        {
                            b=true;
                        }
                    case 2:
                        if(s[i][2]!=v[j][3])
                        {
                            b=true;
                        }
                    case 3:
                        if(s[i][3]>=40)
                        {
                            b=true;
                        }
                }
                if(b)
                {
                    break;
                }
            }
        }
    }
    auto stop = std::chrono::high_resolution_clock::now(); 
    auto duration = std::chrono::duration_cast<std::chrono::microseconds>(stop - start);
    
    for(i=0;i<4;i++)
    {
        op_file<<order[i]<<" ";
    }
    op_file<<std::endl<<duration.count()<<" "<<num_op<<std::endl;
    next_perm(order);
}
\end{lstlisting}
Note that there are $4$ filter operations being executed, that is, there are total $24$ possible ordering. All of which are being executed in the above code and the $"\text{next\_perm}"$ function is creating the next ordering.\\
Here is a part of output generated by the code\\
\begin{lstlisting}
3.32187 0.99998 6.63484 3.29218 2.92342 3.32185 0.99998 6.63482 
1916 1914
0 1 2 3 
171487 3672963
0 1 3 2 
175067 3672963
0 2 1 3 
176562 3672963
0 2 3 1 
175096 3672963
\end{lstlisting}
The first line represents the entropy of the $8$ columns,\\
The next line represents the size of $SegAvgSpeed$ and $SegVol$ respectively.\\
Then for the next 48 lines, it alternates between the order of operations executed and then the time required in microseconds and the number of operations required.\\
For example the above can be interpretted as\\
The entropy of columns are $3.32187,0.99998,6.63484,3.29218,2.92342,3.32185,0.99998,6.63482$\\
The size of $SegAvgSpeed$ is 1916, the size of $SegVol$ is 1914.\\
The order line can be interpretted as the first operation executed is the $0^{th}(\text{S.exp\_way = V.exp\_way})$, second operation executed is $1^{st}(\text{S.dir = V.dir})$, third operation executed is $2^{nd}(\text{S.seg = V.seg})$ and the last operation executed is $3^{rd}(\text{S.speed} <= 40)$, When this order is executed, the time taken is $171487$ mircoseconds and the number of operations required is $3672963$\\
Next 2 lines can be interpretted as the first operation executed is the $0^{th}($$\text{S.exp\_way = V.exp\_way})$, second operation executed is $1^{st}(\text{S.dir = V.dir})$, third operation executed is $3^{rd}(\text{S.speed} <= 40)$ and the last operation executed is $2^{nd}(\text{S.seg = V.seg})$ , When this order is executed, the time taken is $175067$ mircoseconds and the number of operations required is $3672963$. 
\par The fact that the number of operations remains same but the time required changes is due to the internal scheduling by the kernel.
 
\section{Deep Reinforcement Learning}
Now the data for Deep reinforcement learning is stored and ready to be used. For our reinforcement learning it is important to note, any $1$ move will lead us into the final state. So a simple implementation of Deep Neural Networks to estimate the reward function works. And as it is possible that it may be difficult to estimate the rewards, we will check whether the shift on the rewards of various actions is similar so that choosing the best rewarding action based on estimates is still optimal.
\par We consider the sequence/order of moves as an action for our reinforcement learning model, and convert it into a $1$hot encoding.\\
Consider the set of permutations of $\{0,1,2,3\}$, we can order them lexicgraphically.\\
Given a permutation $\mathbb{P}$, it will have a rank $i$ in the ordering. To convert the permutation into a $1$hot encoding, start with a $24$ element vector $v$ with all $0$s, then update $v[i]=1$. Call this resulting vector $\mathbb{V}$.\\
Our DNN will have the training $X$ points as a combination of the entropy, the size of $SegVol$ and $SegAvgSpeed$ and the $1$ hot encoding of the move. In all a vector of length $34$. And the $Y$ coordinate is the number of moves required.\\

\begin{lstlisting}[language=Python]
def reinforcement_learning(fin_train,save_loc,epoch_number):
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Dense(34,activation='relu'))
    model.add(tf.keras.layers.Dense(68,activation='relu'))
    model.add(tf.keras.layers.Dense(136,activation='relu'))
    model.add(tf.keras.layers.Dense(68,activation='relu'))
    model.add(tf.keras.layers.Dense(34,activation='relu'))
    model.add(tf.keras.layers.Dense(1))
    model.compile(optimizer='adam', loss='mse')


    times = int(epoch_number)
    for i in range(times):    
        cm=np.zeros(576).reshape(24,24)
        history=model.fit(x_tr, y2_tr, batch_size=100, epochs=1, callbacks=[cp_callback])
        ans=model.predict(x_te)
        seen=0
        correct=0
        for j in range(len(ans)):
            if(seen==0):
                m1=j
                m2=j
            if(ans[j]<ans[m1]):
                m1=j
            if(y2_te[j]<y2_te[m2]):
                m2=j
            #m1 is the prediction
            #m2 is the actual
            if(seen==23):
                seen=0
                cm[m1%24,m2%24]+=1
                if(m1==m2):
                    correct+=1
            else:
                seen+=1
        total=cm.sum()
        tp = np.asarray([cm[i,i] for i in range(24)])
        fp = cm.sum(axis=1) - tp
        fn = cm.sum(axis=0) - tp
        tn = np.asarray([total-(tp[i]+fp[i]+fn[i]) for i in range(24)]
        for i in range(24):
            try:
                a=(tp[i]+tn[i])/(tp[i]+tn[i]+fn[i]+fp[i])
                p=(tp[i])/(tp[i]+fp[i])
                r=(tp[i])/(tp[i]+fn[i])
                f=(2*p*r)/(p+r)
                print(a,p,r,f)
            except:
                print(i)   
        
        print(len(ans)/24,correct)
        del cm
\end{lstlisting}
The model is trained multiple times and each time we check the number of times the predictions/ estimates lead to an optimal strategy.\\
This is done by comparing the number of operations predicted and the actual number of operations required. If the index with optimal prediction is also index of the the actual optmal value then it is the correct choice.

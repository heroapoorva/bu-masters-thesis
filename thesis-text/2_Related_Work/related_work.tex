\chapter{Related Work}
\label{chapter:related_work}
\thispagestyle{myheadings}

% set this to the location of the figures for this chapter. it may
% also want to be ../Figures/2_Body/ or something. make sure that
% it has a trailing directory separator (i.e., '/')!
\graphicspath{{2_Body/Figures/}}

\section{Introduction, Query optimization}
A database can be thought of as a list of tables, where in each table itself can be considered as a list of data points ordered initially in the sequence they are entered.
\par There are various tools which can be used to connect to a database, here we focus on structured query languages(SQL). A simple SQL query looks like this
\begin{lstlisting}[language=SQL]
    SELECT column_name_1, column_name_2
    FROM table_name
    WHERE condition
\end{lstlisting}
This query is essentially asking to display the 2 columns from the table where the condition given is satisfied. This to particular query might be looking simple, but if the condition introduced is a complex one or if the table from which we need to return the output is complex, the question of how to execute the query optimally becomes difficult to answer.

\section{Converting SQL queries to parse trees}
This step has several functions.
\par If a "view" is used in the query as a relation, then each instance has to be replaced by the parse tree.
\par The preprocessor also has to conduct semaContainment of Value Sets. If Y is an attribute appearing in several rela足
tions, then each relation chooses its values from the front of a fixed list of
values y1 , y2 , y3 ,
and has all the values in that prefix. As a consequence,
if R and S are two relations with an attributContainment of Value Sets. If Y is an attribute appearing in several rela足
tions, then each relation chooses its values from the front of a fixed list of
values y1 , y2 , y3 ,
and has all the values in that prefix. As a consequence,
if R and S are two relations with an attribute Y, and V(R, Y) ::; V(S, Y ),
then every Y-value o f R will be a Y-value of S.e Y, and V(R, Y) ::; V(S, Y ),
then every Y-value o f R will be a Y-value of S.ntic checking, that is, check if relations used exist, check for ambiguity, and type checking.
If a parse tree passes the preprocessing then it is said to be \textbf{valid}
We don't describe the exact grammar for the convertion to the parse tree. In these parse trees, there are 2 types of nodes, one the atoms, which are essentially keywords in SQL, operators, constants and attributes. The second is Syntactic categories, these are names for families of subqueries in triangular brackets. Each of the syntactic category has unique expansion into atoms and further syntactic categories.


\section{Relational algebra}
As we saw above, order of operations matters, if the order of operations is not thoughtout and done blindly alot of redundant steps are executed and memory is moved around unnecessarily. There are few ways to atleast look and analyse the operations and how they can be simplied.
\par Let \textsc{R,S} be relations. Some simple laws, associativity and commutativity can easily be verified:-
\begin{itemize}
    \item $R\times S = S \times R$
    \item $(R\times S) \times T = R\times (S \times T)$
    \item $R \bowtie S = S \bowtie R$
    \item $(R \bowtie S) \bowtie T = R \bowtie (S \bowtie T)$
    \item $R \cup S = S \cup R$
    \item $(R \cup S) \cup T = R \cup (S \cup T)$
    \item $R \cap S = S \cap R$
    \item $(R \cap S) \cap T = R \cap (S \cap T)$
\end{itemize}
When applying associative law on relations, need to be careful whether the conditions actually makes sense after the order is changed.
\par While the above identities work on both sets and bags(bags allow for repeatition). To show that laws for sets and bags do differ an easy way is to consider the distributive property.
\begin{center}
$A\cap _S(B\cup_S C) = (A\cap_S B)\cup_S(A\cap_S C)$\\
$A\cap _B(B\cup_B C) \neq (A\cap_B B)\cup_B(A\cap_B C)$
\end{center}
We can simply show it with an example. Let $A=\{t\},B=\{t\},C=\{t\}$. The LHS comes to be $\{t\}$, whereas RHS is $\{t,t\}$

\subsection{Select operator $\sigma$}
First we start with simple properties of the $\sigma$ operator. Need to be careful about the attributes used in the select operator condition when pushing it down.
\begin{itemize}
    \item $\sigma_{C_1 \land C_2}(R) = \sigma_{C_1}(\sigma_{C_2}(R)) $
    \item $\sigma_{C_1 \lor C_2}(R) = (\sigma_{C_1}(R)) \cup_S (\sigma_{C_2}(R))$
    \item $\sigma_{C}(R \cup S) = \sigma_{C}(R) \cup \sigma_{C}(S)$
    \item $\sigma_{C}(R - S) = \sigma_{C}(R) - \sigma_{C}(S) = \sigma_{C}(R) - S$
    \item $\sigma_{C}(R \times S) = \sigma_{C}(R) \times S$
    \item $\sigma_{C}(R \bowtie S) = \sigma_{C}(R) \bowtie S$
    \item $\sigma_{C}(R \underset{D}{\bowtie} S) = \sigma_{C}(R) \underset{D}{\bowtie} S$
    \item $\sigma_{C}(R \cap S) = \sigma_{C}(R) \cap S$
\end{itemize}

\subsection{Projection operator $\pi$}
While for the Select operator($\sigma$) the identities were quite straight forward with not many things to consider, the identities for Projection operator ($\pi$) are bit more involved.
\begin{itemize}
    \item $\pi_{L}(R \bowtie S) = \pi_{L}(\pi_{M}(R) \bowtie \pi_{N}(S))$, where $M,N$ are attributes required for the join or they are inputs to the projection.
    \item $\pi_{L}(R \underset{D}{\bowtie} S) = \pi_{L}(\pi_{M}(R) \underset{D}{\bowtie} \pi_{N}(S))$, similar to above identity/ law.
    \item $\pi_{L}(R \times S) = \pi_{L}(\pi_{M}(R) \times \pi_{N}(S))$
    \item $\pi_{L}(R \cup_{B} S) = \pi_{L}(R) \cup_{B} \pi_{L}(S)$
    \item $\pi_{L}(\sigma_{C}(R))=\pi_{L}(\sigma_{C}(\pi_{M}(R))$
\end{itemize}

\subsection{Duplicate Elimination operator $\delta$}
The $\delta$ operator eliminates duplicates from bags. 
\begin{itemize}
    \item $\delta(R)= R,$ if $R$ does not have any duplicates.
    \item $\delta(R \times S) = \delta(R) \times \delta(S)$
    \item $\delta(R \bowtie S) = \delta(R) \bowtie \delta(S)$
    \item $\delta(R \underset{D}{\bowtie} S) = \delta(R) \underset{D}{\bowtie} \delta(S)$
    \item $\delta(\sigma_{C}(R)) = \sigma_{C}(\delta(R))$
    \item $\delta(R \cap_{B} S) = \delta(R) \cap_{B} S$
\end{itemize}

\subsection{Aggregation operator $\gamma$}
It is difficult to give identities for the aggregation operator, like done for the above operators. This is mostly due to how the details of how the aggregation operator is used.
\begin{itemize}
    \item $\sigma(\gamma_{L}(R)) = \gamma_{L}(R)$
    \item $\gamma_{L}(R) = \gamma_{L}(\pi_{M}(R)),$ where $M$ must at least contain the attributed used in $L$.
\end{itemize}
\section{Converting Parse trees into logical expression}
Till now, the only SQL related information presented is how to convert a Query into the parse tree, which is grammar dependent. Given the parse tree, need to substitute nodes by operators seen above, later this expression is optimized to be later converted to a physical query plan.
\par Now to convert the parse tree into the logical expression. First, look at the transformation of select-from-where statement.
\begin{itemize}
\item $<Query> \rightarrow <SFW>.$
\item $<SFW> \rightarrow SELECT <SelList> FROM <FromList> WHERE <Condition>.$
\item $<SelList> \rightarrow \pi_{L}$, where $L$ is the list of attributes in $<SelList>.$
\item $<Condition> \rightarrow \sigma_{C},$ where $C$ is the equivalent of $<Condition>$ 
\end{itemize}
While there is nothign inherently wrong about the last statement, need to consider the case where $<Condition>$ involves subqueries. A simple explanation about why it isn't allowed is a convention normally the subscript has to be a boolean condition, and if it was allowed otherwise, it would be a very expensive operation, as the subscript in the select operator has to evaluted at every element of the argument relation. This shows the redundance of it. While if this is allowed, it can be simplified and made efficient, but has to be done on a case by case basis with the use of $\bowtie, \times$ functions.
\par Overall it is a good idea to not use subquerying and rather using joins.
\par At this point, by making the substitutions mentioned above and using the algebraic identities, we obtain a starting logical query plan. The query has to be transformed into a query which the compiler believes to be the cheapest or the optimal. But, a thing which further complicates the process is the join order.
\par With the current knowledge, few optimizing rules are evident.
\begin{itemize}
    \item \textbf{Selection repositioning}, Selections should be pushed down as much as possbile, but sometimes, might need to take the selection operator a level up first.
    \item Pushing projections down the parse tree, being careful with the new projections made in the process.
    \item Duplicate removal needs to be repositioned.
    \item $\sigma$ combined with $\times$ below can result in equijoin, which is much more efficient. 
\end{itemize}
\par Normally, parsers only have nodes with $0, 1, 2$ children, which corresponds to uniary and binary operators. But many of the operators(natural join, union, and intersection) are commutative and associative, so it helps combine them on a single level to provide the opportunity to prioritize which ones are done first. To do this step, the following guidelines are sufficient.
\begin{itemize}
    \item We must replace the natural joins with theta-joins that equate the at足 tributes of the same name.
    \item We must add a projection to eliminate duplicate copies of attributes in足volved in a natural join that has become a theta-join
    \item The theta-join conditions must be associative.
\end{itemize}
\section{Explain difficulties/ Time complexity}
Currently, we have taken a query as an input and converted it into a logical plan, and then applied more transformation using relational algebra to make the optimal query plan. The next step is to convert it into a physical query plan which can be executed. To complete this step, need to compare the cost of various physical query plan derived from the logical query plan. This plan with the least cost is then passed query execution engine and executed. When trying to select a query plan, need to select:-
\begin{itemize}
    \item An order and grouping for associative-and-commutative operations like
joins, unions, and intersections.
    \item An algorithm for each operator in the logical plan, for instance, deciding whether a nested-loop join or a hash-join should be used.
    \item Additional operators - scanning, sorting, and so on that are needed for the physical plan but that were not present explicitly in the logical plan.
    \item The way in which arguments are passed from one operator to the next, for instance, by storing the intermediate result on disk or by using iterators and passing an argument one tuple or one main-memory buffer at a time.
\end{itemize}
But after selecting these for a physical plan, one obivious way to calculate the time is actually executing the plan to see the cost, but that way essentially every plan is carried out. That is expensive and redundant. Is there a better way?

\subsection{Estimating size and cost}
$B(R) \coloneqq$ is the number of blocks needed to hold all the tuples of relation $R$.\\
$T(R) \coloneqq$ is the number of tuples of relation $R$.\\
$V(R,a) \coloneqq$ is the value count for attribute a of relation $R$, that is, the number of distinct values relation $R$ has in attribute $a$.\\
$V(R, [ a_1 , a_2,..., a_n]) \coloneqq$ is the number of distinct values $R$ has when all of attributes $a_1, a_2,..., a_n$ are considered together, that is, the number of tuples in $\delta(\pi_{a_1,a_2,...,a_n}(R))$
\par Need to remember that physical plan is selected to minimize the estimated cost of evaluating the query and that intermidate/ temporary relations calculated will also incur a cost on the system. So need to take them into account as well. Over all the estimation method should pass the following sanity check:-
\begin{itemize}
    \item Give accurate estimates. No matter what method is used for executing query plans.
    \item Are easy to compute.
    \item Are logically consistent; that is, the size estimate for an intermediate re足lation should not depend on how that relation is computed. For instance, the size estimate for a join of several relations should not depend on the order in which we join the relations.
\end{itemize}
But there is no agreed upon method to do this, but the goal is to help select a query plan and not to find the exact minimum. So even if the estimated cost is wrong, but if it is wrong similarly then we will still have the least costing plan.

\subsection{Estimation of Projection}
The projection operator is a bag operation, so it does not reduce the number of tuples, only reduces the size of each tuple.\\
$T(R) = T(\pi(R))$\\
$B(R) \leq B(\pi(R))$

\subsection{Estimation of Selection}
Let $S = \sigma_{A=c}(R)$, here $A$ is an attribute of $R$ and $c$ is a constant\\
$T(S)=\frac{T(R)}{V(R,A)}$, is it important to note that this is an estimate and not the actual value, this will be the actual value if all the attributes in $A$ have equal occurance. An even better estimate can be obtained if the DBMS stores a statistic know as histogram. 
\par The above calculate was easy because of the equality. if $S=\sigma_{A < c}(R)$, we simply estimate $T(S)=\frac{T(R)}{3}$
\par Now if $S=\sigma_{A\neq c}(R)$, can take $T(S)=T(R)$ or $T(S)= T(R) - \frac{T(R)}{V(R,A)}$
\par If $S=\sigma_{A = c_1 \lor c_2}$, take them to be independent conditions.

\subsection{Estimation of Join, single attribute}
For natural joins, we assume, the natural join of two relations involves only the equality of two attributes. That is, we study the
join $R(X, Y) \bowtie S(Y, Z)$ , but initially we assume that $Y$ is a single attribute although X and Z can represent any set of attributes. But it is hard to find a good estimate as $T(R\bowtie S) \in [0, T(R)*T(S)]$, to help with this two assumptions are made.
\begin{itemize}
    \item \textbf{Containment of Value Sets} If Y is an attribute appearing in several rela足tions, then each relation chooses its values from the front of a fixed list of values $y_1, y_2, y_3,...$ and has all the values in that prefix. As a consequence, if $R$ and $S$ are two relations with an attribute $Y$, and $V(R, Y) \leq V(S, Y)$, then every $Y$-value of $R$ will be a $Y$-value of $S$. 
    \item \textbf{Preservation of Value Sets} If we join a relation $R$ with another relation, then an attribute A that is not a join attribute (i.e., not present in both relations) does not lose values from its set of possible values. More precisely, if $A$ is an attribute of $R$ but not of $S$, then $V(R \bowtie S, A) = V(R, A)$. Note that the order of joining $R$ and $S$ is not important, so we could just as well have said that $V(S \bowtie R, A) = V (R, A)$.
\end{itemize}
\par Using the above assumptions we can claim, 
$$T(R\bowtie S)=\frac{T(R)*T(S)}{max(V(R,Y), V(S,Y))}$$
Let $V(R,Y) \leq V(S,Y)$, then every tuple $t$ of $R$ has a chance of $\frac{1}{V(S,Y)}$ of joining with a given tuple of $S$. Since there are $T(S)$ tuples in $S$, the expected number of tuples that $t$ joins with is $\frac{T(S)}{V(S, Y)}$. As there are
$T(R)$ tuples of $R$, the estimated size of $R \bowtie S$ is $\frac{T(R)*T(S)}{V(S,Y)}$, if it was $V(R,Y) \geq V(S,Y)$, then $\frac{T(R)*T(S)}{V(R,Y)}$
\par Guidelines for other type of joins:-
\begin{itemize}
    \item The number of tuples in the result of an equijoin can be computed exactly as for a natural join, after accounting for the change in variable names.
    \item Other theta-joins can be estimated as if they were a selection following a product.
\end{itemize}


\section{Introduction to Data Streams}


\section{Data stream windowing}


\section{Query Processing of data streams(Combine the DBMS and DSMS)}


\section{Challenges of query optimization on data streams}


\section{Conclusion and discussion}



\begin{comment}

\section{SQL Query compiler}
The steps involved are
\subsection{Parsing}
In a very general sense, given an SQL query, SQL converts it into a parse tree based on SQL grammar.
\subsection{Logical Query Plan}
The first step is to modify the parse tree into using operators and operators of relational algebra.
\par The next step is to convert expression obtained from the above substitution and modify it into an expression which can be converted to most efficient physical query plan.
\par To improve the algebraic expression obtained, few common steps taken are pushing down selections and projections carefully, carefully placing duplicate eliminations, combining selections, showing associativity and commutivity in the expression to help with enumeration.
\par At the end when we have the expression ready, we enumerate the physical plans and calculate their cost of execution and select the method with the lowest cost.
\subsection{Cost Estimation}
We need to consider what algorithm each operator in the expression is going to use, such as join, sort, scanning and more. Also need to consider the order for the associative and commutative operators, because at the end the operators are binary and how the output of one operator is provided as an input to the next/ outter operator in the expression. 
\section{System R}
\label{sec:systemR}
\section{Deep Reinforcement learning}
\label{sec:drl}

Markov decision process(MDP) is used to formalize various types of stochastic processes. In MDPs, the goal of the agent is to make a sequence of actions to optimize/ maximize an objective function. \\
Formally a MDP is a $5-$tuple 
\begin{center}
$\langle S,A,P(s,a),R(s,a), s_0 \rangle $\\
$S \to $ Set of all possible states the agent can be in.\\
$A \to $ Set of all possible actions the agent can take.\\
$P(s,a) \to $ A probability distribution of going to various states given current state and action. $s^{1} \sim P(s,a)$\\
$R(s,a) \to $ Reward for taking action $a$ on state $s$.\\
$s_0 \to $ Describes the initial state of the system/ agent.
\end{center}
The performance of the agent is measured using the rewards collected along the way through various states. So the objective of an MDP is to find a policy $\pi:S\rightarrow A$, a function that maps states to actions, in order to maximize the expected value:-
\begin{center}
    \[ 
    \argmax_{\pi} \mathbb{E} \left[ \sum^{T-1}_{t=0}R(s_{t},a_{t})  \right]
    \] 
    subject to $s_{t+1} = P(s_{t},a_{t}), a_{t} = \pi{s_{t}}$
\end{center}
This method does not reduce the search space, and unlikely greedy solution, this will lead to an optimal solution. This method does not reduce the search space, and unlikely greedy solution, this will lead to an optimal solution.
\par Reinforcement learning(RL) is a technique which optimizes MDPs iteratively, by running a simulation in each iteration and changing the policy to find an optimal one based on the cumulative reward.
\section{Relations}
\label{sec:relations}
A common method/ data structure used to formalize joins
\par \textbf{Query Graph $\to$} A query graph $G$ is an undirected graph, where each relation R is a vertex and each join predicate $\rho$ defines an edge between $2$ vertices. Let $\kappa_G$ denote the number of connected components in $G$
\par A join of relation $R_1, R_2$, in the graph corresponds to remove the vertices $v_{R_1}, v_{R_2}$, replacing them with a vertex $v_{R_1+R_2}$, the edges of the form $(v_{R_1},v) \& (v_{R_2},v)$ are replaced by $(v_{R_1+R_2}, v)$. Note each reduction reduces number of vertices by one, so this process is repeated until there are $\kappa_G$ number of vertices left.
\par \textbf{Join Optimization Problem $\to$} Let $G$ be a query graph and $J$ be a join cost model. Find sequence, $c_1\circ c_2\circ ... \circ c_n$ resulting in $|V| = \kappa_G$ to minimize
\begin{center}
    \[
        \min_{c_1,c_2,...c_n} \sum^{n}_{i=1}J(c_i)
    \] 
    subject to $G_{i+1} = c(G_i)$
\end{center}
Using these definitions, we define a MDP. 
\begin{center}
$\langle \{G_0,G_1,...G_T\},c,P(G,c),-J, G \rangle $\\
\end{center}
We are still not certain about how the cost function $J$ is structured. We are still not certain about how the cost function $J$ is structured. We are still not certain about how the cost function $J$ is structured.

We are still not certain about how the cost function $J$ is structured.

\end{comment}








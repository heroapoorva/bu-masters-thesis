\chapter{Related Work}
\label{chapter:related_work}
\thispagestyle{myheadings}

% set this to the location of the figures for this chapter. it may
% also want to be ../Figures/2_Body/ or something. make sure that
% it has a trailing directory separator (i.e., '/')!
\graphicspath{{2_Body/Figures/}}

\section{Introduction, Query optimization}
A database can be thought of as a list of tables, where in each table itself can be considered as a list of data points ordered initially in the sequence they are entered.
\par There are various tools which can be used to connect to a database, here we focus on structured query languages(SQL). A simple SQL query looks like this
\begin{lstlisting}[language=SQL]
    SELECT column_name_1, column_name_2
    FROM table_name
    WHERE condition
\end{lstlisting}
This query is essentially asking to display the 2 columns from the table where the condition given is satisfied. This to particular query might be looking simple, but if the condition introduced is a complex one or if the table from which we need to return the output is complex, the question of how to execute the query optimally becomes difficult to answer.

\section{Converting SQL queries to parse trees}
This step has several functions.
\par If a "view" is used in the query as a relation, then each instance has to be replaced by the parse tree.
\par The preprocessor also has to conduct semaContainment of Value Sets. If Y is an attribute appearing in several rela­
tions, then each relation chooses its values from the front of a fixed list of
values y1 , y2 , y3 ,
and has all the values in that prefix. As a consequence,
if R and S are two relations with an attributContainment of Value Sets. If Y is an attribute appearing in several rela­
tions, then each relation chooses its values from the front of a fixed list of
values y1 , y2 , y3 ,
and has all the values in that prefix. As a consequence,
if R and S are two relations with an attribute Y, and V(R, Y) ::; V(S, Y ),
then every Y-value o f R will be a Y-value of S.e Y, and V(R, Y) ::; V(S, Y ),
then every Y-value o f R will be a Y-value of S.ntic checking, that is, check if relations used exist, check for ambiguity, and type checking.
If a parse tree passes the preprocessing then it is said to be \textbf{valid}
We don't describe the exact grammar for the convertion to the parse tree. In these parse trees, there are 2 types of nodes, one the atoms, which are essentially keywords in SQL, operators, constants and attributes. The second is Syntactic categories, these are names for families of subqueries in triangular brackets. Each of the syntactic category has unique expansion into atoms and further syntactic categories.


\section{Relational algebra}
As we saw above, order of operations matters, if the order of operations is not thoughtout and done blindly alot of redundant steps are executed and memory is moved around unnecessarily. There are few ways to atleast look and analyse the operations and how they can be simplied.
\par Let \textsc{R,S} be relations. Some simple laws, associativity and commutativity can easily be verified:-
\begin{itemize}
    \item $R\times S = S \times R$
    \item $(R\times S) \times T = R\times (S \times T)$
    \item $R \bowtie S = S \bowtie R$
    \item $(R \bowtie S) \bowtie T = R \bowtie (S \bowtie T)$
    \item $R \cup S = S \cup R$
    \item $(R \cup S) \cup T = R \cup (S \cup T)$
    \item $R \cap S = S \cap R$
    \item $(R \cap S) \cap T = R \cap (S \cap T)$
\end{itemize}
When applying associative law on relations, need to be careful whether the conditions actually makes sense after the order is changed.
\par While the above identities work on both sets and bags(bags allow for repeatition). To show that laws for sets and bags do differ an easy way is to consider the distributive property.
\begin{center}
$A\cap _S(B\cup_S C) = (A\cap_S B)\cup_S(A\cap_S C)$\\
$A\cap _B(B\cup_B C) \neq (A\cap_B B)\cup_B(A\cap_B C)$
\end{center}
We can simply show it with an example. Let $A=\{t\},B=\{t\},C=\{t\}$. The LHS comes to be $\{t\}$, whereas RHS is $\{t,t\}$

\subsection{Select operator $\sigma$}
First we start with simple properties of the $\sigma$ operator. Need to be careful about the attributes used in the select operator condition when pushing it down.
\begin{itemize}
    \item $\sigma_{C_1 \land C_2}(R) = \sigma_{C_1}(\sigma_{C_2}(R)) $
    \item $\sigma_{C_1 \lor C_2}(R) = (\sigma_{C_1}(R)) \cup_S (\sigma_{C_2}(R))$
    \item $\sigma_{C}(R \cup S) = \sigma_{C}(R) \cup \sigma_{C}(S)$
    \item $\sigma_{C}(R - S) = \sigma_{C}(R) - \sigma_{C}(S) = \sigma_{C}(R) - S$
    \item $\sigma_{C}(R \times S) = \sigma_{C}(R) \times S$
    \item $\sigma_{C}(R \bowtie S) = \sigma_{C}(R) \bowtie S$
    \item $\sigma_{C}(R \underset{D}{\bowtie} S) = \sigma_{C}(R) \underset{D}{\bowtie} S$
    \item $\sigma_{C}(R \cap S) = \sigma_{C}(R) \cap S$
\end{itemize}

\subsection{Projection operator $\pi$}
While for the Select operator($\sigma$) the identities were quite straight forward with not many things to consider, the identities for Projection operator ($\pi$) are bit more involved.
\begin{itemize}
    \item $\pi_{L}(R \bowtie S) = \pi_{L}(\pi_{M}(R) \bowtie \pi_{N}(S))$, where $M,N$ are attributes required for the join or they are inputs to the projection.
    \item $\pi_{L}(R \underset{D}{\bowtie} S) = \pi_{L}(\pi_{M}(R) \underset{D}{\bowtie} \pi_{N}(S))$, similar to above identity/ law.
    \item $\pi_{L}(R \times S) = \pi_{L}(\pi_{M}(R) \times \pi_{N}(S))$
    \item $\pi_{L}(R \cup_{B} S) = \pi_{L}(R) \cup_{B} \pi_{L}(S)$
    \item $\pi_{L}(\sigma_{C}(R))=\pi_{L}(\sigma_{C}(\pi_{M}(R))$
\end{itemize}

\subsection{Duplicate Elimination operator $\delta$}
The $\delta$ operator eliminates duplicates from bags. 
\begin{itemize}
    \item $\delta(R)= R,$ if $R$ does not have any duplicates.
    \item $\delta(R \times S) = \delta(R) \times \delta(S)$
    \item $\delta(R \bowtie S) = \delta(R) \bowtie \delta(S)$
    \item $\delta(R \underset{D}{\bowtie} S) = \delta(R) \underset{D}{\bowtie} \delta(S)$
    \item $\delta(\sigma_{C}(R)) = \sigma_{C}(\delta(R))$
    \item $\delta(R \cap_{B} S) = \delta(R) \cap_{B} S$
\end{itemize}

\subsection{Aggregation operator $\gamma$}
It is difficult to give identities for the aggregation operator, like done for the above operators. This is mostly due to how the details of how the aggregation operator is used.
\begin{itemize}
    \item $\sigma(\gamma_{L}(R)) = \gamma_{L}(R)$
    \item $\gamma_{L}(R) = \gamma_{L}(\pi_{M}(R)),$ where $M$ must at least contain the attributed used in $L$.
\end{itemize}
\section{Converting Parse trees into logical expression}
Till now, the only SQL related information presented is how to convert a Query into the parse tree, which is grammar dependent. Given the parse tree, need to substitute nodes by operators seen above, later this expression is optimized to be later converted to a physical query plan.
\par Now to convert the parse tree into the logical expression. First, look at the transformation of select-from-where statement.
\begin{itemize}
\item $<Query> \rightarrow <SFW>.$
\item $<SFW> \rightarrow SELECT <SelList> FROM <FromList> WHERE <Condition>.$
\item $<SelList> \rightarrow \pi_{L}$, where $L$ is the list of attributes in $<SelList>.$
\item $<Condition> \rightarrow \sigma_{C},$ where $C$ is the equivalent of $<Condition>$ 
\end{itemize}
While there is nothign inherently wrong about the last statement, need to consider the case where $<Condition>$ involves subqueries. A simple explanation about why it isn't allowed is a convention normally the subscript has to be a boolean condition, and if it was allowed otherwise, it would be a very expensive operation, as the subscript in the select operator has to evaluted at every element of the argument relation. This shows the redundance of it. While if this is allowed, it can be simplified and made efficient, but has to be done on a case by case basis with the use of $\bowtie, \times$ functions.
\par Overall it is a good idea to not use subquerying and rather using joins.
\par At this point, by making the substitutions mentioned above and using the algebraic identities, we obtain a starting logical query plan. The query has to be transformed into a query which the compiler believes to be the cheapest or the optimal. But, a thing which further complicates the process is the join order.
\par With the current knowledge, few optimizing rules are evident.
\begin{itemize}
    \item \textbf{Selection repositioning}, Selections should be pushed down as much as possbile, but sometimes, might need to take the selection operator a level up first.
    \item Pushing projections down the parse tree, being careful with the new projections made in the process.
    \item Duplicate removal needs to be repositioned.
    \item $\sigma$ combined with $\times$ below can result in equijoin, which is much more efficient. 
\end{itemize}
\par Normally, parsers only have nodes with $0, 1, 2$ children, which corresponds to uniary and binary operators. But many of the operators(natural join, union, and intersection) are commutative and associative, so it helps combine them on a single level to provide the opportunity to prioritize which ones are done first. To do this step, the following guidelines are sufficient.
\begin{itemize}
    \item We must replace the natural joins with theta-joins that equate the at­ tributes of the same name.
    \item We must add a projection to eliminate duplicate copies of attributes in­volved in a natural join that has become a theta-join
    \item The theta-join conditions must be associative.
\end{itemize}


\section{Explain difficulties/ Time complexity}
Currently, we have taken a query as an input and converted it into a logical plan, and then applied more transformation using relational algebra to make the optimal query plan. The next step is to convert it into a physical query plan which can be executed. To complete this step, need to compare the cost of various physical query plan derived from the logical query plan. This plan with the least cost is then passed query execution engine and executed. When trying to select a query plan, need to select:-
\begin{itemize}
    \item An order and grouping for associative-and-commutative operations like
joins, unions, and intersections.
    \item An algorithm for each operator in the logical plan, for instance, deciding whether a nested-loop join or a hash-join should be used.
    \item Additional operators - scanning, sorting, and so on that are needed for the physical plan but that were not present explicitly in the logical plan.
    \item The way in which arguments are passed from one operator to the next, for instance, by storing the intermediate result on disk or by using iterators and passing an argument one tuple or one main-memory buffer at a time.
\end{itemize}
But after selecting these for a physical plan, one obivious way to calculate the time is actually executing the plan to see the cost, but that way essentially every plan is carried out. That is expensive and redundant. Is there a better way?

\subsection{Estimating size and cost}
$B(R) \coloneqq$ is the number of blocks needed to hold all the tuples of relation $R$.\\
$T(R) \coloneqq$ is the number of tuples of relation $R$.\\
$V(R,a) \coloneqq$ is the value count for attribute a of relation $R$, that is, the number of distinct values relation $R$ has in attribute $a$.\\
$V(R, [ a_1 , a_2,..., a_n]) \coloneqq$ is the number of distinct values $R$ has when all of attributes $a_1, a_2,..., a_n$ are considered together, that is, the number of tuples in $\delta(\pi_{a_1,a_2,...,a_n}(R))$
\par Need to remember that physical plan is selected to minimize the estimated cost of evaluating the query and that intermidate/ temporary relations calculated will also incur a cost on the system. So need to take them into account as well. Over all the estimation method should pass the following sanity check:-
\begin{itemize}
    \item Give accurate estimates. No matter what method is used for executing query plans.
    \item Are easy to compute.
    \item Are logically consistent; that is, the size estimate for an intermediate re­lation should not depend on how that relation is computed. For instance, the size estimate for a join of several relations should not depend on the order in which we join the relations.
\end{itemize}
But there is no agreed upon method to do this, but the goal is to help select a query plan and not to find the exact minimum. So even if the estimated cost is wrong, but if it is wrong similarly then we will still have the least costing plan.

\subsection{Estimation of Projection}
The projection operator is a bag operation, so it does not reduce the number of tuples, only reduces the size of each tuple.\\
$T(R) = T(\pi(R))$\\
$B(R) \leq B(\pi(R))$

\subsection{Estimation of Selection}
Let $S = \sigma_{A=c}(R)$, here $A$ is an attribute of $R$ and $c$ is a constant\\
$T(S)=\frac{T(R)}{V(R,A)}$, is it important to note that this is an estimate and not the actual value, this will be the actual value if all the attributes in $A$ have equal occurance. An even better estimate can be obtained if the DBMS stores a statistic know as histogram. 
\par The above calculate was easy because of the equality. if $S=\sigma_{A < c}(R)$, we simply estimate $T(S)=\frac{T(R)}{3}$
\par Now if $S=\sigma_{A\neq c}(R)$, can take $T(S)=T(R)$ or $T(S)= T(R) - \frac{T(R)}{V(R,A)}$
\par If $S=\sigma_{A = c_1 \lor c_2}$, take them to be independent conditions.

\subsection{Estimation of Join, single attribute}
For natural joins, we assume, the natural join of two relations involves only the equality of two attributes. That is, we study the
join $R(X, Y) \bowtie S(Y, Z)$ , but initially we assume that $Y$ is a single attribute although X and Z can represent any set of attributes. But it is hard to find a good estimate as $T(R\bowtie S) \in [0, T(R)*T(S)]$, to help with this two assumptions are made.
\begin{itemize}
    \item \textbf{Containment of Value Sets} If Y is an attribute appearing in several rela­tions, then each relation chooses its values from the front of a fixed list of values $y_1, y_2, y_3,...$ and has all the values in that prefix. As a consequence, if $R$ and $S$ are two relations with an attribute $Y$, and $V(R, Y) \leq V(S, Y)$, then every $Y$-value of $R$ will be a $Y$-value of $S$. 
    \item \textbf{Preservation of Value Sets} If we join a relation $R$ with another relation, then an attribute A that is not a join attribute (i.e., not present in both relations) does not lose values from its set of possible values. More precisely, if $A$ is an attribute of $R$ but not of $S$, then $V(R \bowtie S, A) = V(R, A)$. Note that the order of joining $R$ and $S$ is not important, so we could just as well have said that $V(S \bowtie R, A) = V (R, A)$.
\end{itemize}
\par Using the above assumptions we can claim, 
$$T(R\bowtie S)=\frac{T(R)*T(S)}{max(V(R,Y), V(S,Y))}$$
Let $V(R,Y) \leq V(S,Y)$, then every tuple $t$ of $R$ has a chance of $\frac{1}{V(S,Y)}$ of joining with a given tuple of $S$. Since there are $T(S)$ tuples in $S$, the expected number of tuples that $t$ joins with is $\frac{T(S)}{V(S, Y)}$. As there are
$T(R)$ tuples of $R$, the estimated size of $R \bowtie S$ is $\frac{T(R)*T(S)}{V(S,Y)}$, if it was $V(R,Y) \geq V(S,Y)$, then $\frac{T(R)*T(S)}{V(R,Y)}$
\par Guidelines for other type of joins:-
\begin{itemize}
    \item The number of tuples in the result of an equijoin can be computed exactly as for a natural join, after accounting for the change in variable names.
    \item Other theta-joins can be estimated as if they were a selection following a product.
\end{itemize}

\subsection{Estimation of Join, multiple attribute}
Now we assume $Y$ in $R(X, Y) \bowtie S(Y, Z)$ represents multiple attributes. Say for example, $R(X, y_1, y_2) \bowtie S(y_1, y_2, Z)$. We again do a probability calculation.
\par Let $r \in R, s \in S$ be a tuples, the probability that $r,s$ agree on $y_1$ is $\frac{1}{max(V(R,y_1), V(S,y_1))}$, similarly for $y_2$. So we get the expected value to be
$$\frac{T(R)*T(S)}{max(V(R,y_1), V(S,y_1))*max(V(R,y_2), V(S,y_2))}$$ 
From this the pattern is clear, need to divide $T(R)*T(S)$ by maximum of $V(R,y), V(S,y)$ for each attribute.
\par But this is only the calculation for $T(R\bowtie S)$, need to calculate for $B(R\bowtie S)$ as well.

\subsection{Multiple Joins}
For this case we work with $S = R_1 \bowtie R_2 \bowtie ... \bowtie R_n$
\par Here we have to make use of the containment assumption. Say the attribute $A$ appears in $k$ of $R_i$'s and the values corresponding to $V(R_i,A)$ are $v_1 \leq v_2 \leq ... \leq v_k$, need to find the probability that tuples agree on $A$.
\par Consider the tuple $t_1$ chosen from the relation that has the small­est number of $A-$values, $v_1$ . By the containment assumption, each of these $v_1$ values is among the $A-$values found in the other relations that have attribute $A$. Consider the relation that has $V_i$ values in attribute $A$. Its selected tuple $t_i$ has probability $\frac{1}{v_1}$ of agreeing with $t_1$ on $A$. Since this claim is true for all $i \in \{2, 3,..., k\}$, the probability that all $k$ tuples agree on $A$ is the product $\frac{1}{v_2 * v_3 *...* V_k}$. This analysis gives us the rule for estimating the size of any join.
\par Start with the product of the number of tuples in each relation. Then for each attribute A appearing at least twice, divide by all but the least of the $V(R, A)$'s

\subsection{Union}
If $U_B$ is used, it is the sum of the two individually. 
\par If $U_S$ is used, then number of tuples range from the max of two, to their sum. So take mean of the range.

\subsection{Intersection}
The number of tuples here ranges from $0$ to the minimum of the two(in case of set intersection), so again can take the mean of this range.

\subsection{Difference}
The range for $R-S$ is $[T(R)-T(S),T(R)]$, so again mean of the range.

\subsection{Duplicate Elimination}
The range for $\delta(R)$ is $[1, T(R)]$, so again can take the mean, there can be other estimates as well.
\par A nice compromise is $min(\frac{T(R)}{2}, \prod V(R,a_i))$

\subsection{Grouping and Aggregation}
Same as duplicate.


\section{Other tools}
We assume that the "cost" of evaluating an expression is approximated well by the number of disk I/O's performed. The number of disk I/O's, in turn, is influenced by:
\begin{itemize}
    \item The particular logical operators chosen to implement the query, a matter decided when we choose the logical query plan.
    \item The sizes of intermediate results.
    \item The physical operators used to implement logical operators, e.g., the choice of a one-pass or two-pass join, or the choice to sort or not sort a given relation.
    \item The ordering of similar operations, especially joins.
    \item The method of passing arguments from one physical operator to the next.
\end{itemize}

\subsection{Histogram}
In the earlier section the statistics were heavily used in calculations. Another statistic that can be stored is the histogram.
\par If $V (R, A)$ is not too large, then the histogram may consist of the number (or fraction) of the tuples having each of the values of attribute $A$. If there are a great many values of this attribute, then only the most frequent values may be recorded individually, while other values are counted in groups.
\par \textbf{Equal-width} select 2 parameters, $w$ the width and $v_0$ a begining point of a column in the histogram, which will initally be the considered the lower bound and if an even lower value is noticed, make a smaller column as well and update the lower bound.
\par \textbf{Equal-height} These are the common "percentiles". A percentile $p, 2p$, so on.
\par \textbf{Most-frequent-values} List the most common values and their numbers of occurrences. This information may be provided along with a count of occurrences for all the other values as a group, or we may record frequent values in addition to an equal-width or equal-height histogram for the other values.

\subsection{Heuristics}
One important use of cost estimates for queries or subqueries is in the appli­cation of heuristic transformations of the query.
\par Heuristics applied independent of cost estimates can be expected almost certainly to improve the cost of a logical query plan
\par However, there are other points in the query optimization process where es­timating the cost both before and after a transformation will allow us to apply a transformation where it appears to reduce cost and avoid the transformation otherwise. In particular, when the preferred logical query plan is being generated, we may consider a number of optional transformations and the costs before and after. Because we are estimating the cost of a logical query plan, so we have not yet made decisions about the physical operators that will be used to implement the operators of relational algebra, our cost estimate cannot be based on disk I / O's. Rather, we estimate the sizes of all intermediate results their sum is our heuristic estimate for the cost of the entire logical plan.

\section{Enumeration Methods}
The naive method of find the least costing plan is enumerating all the possible plans and calculating the cost for them and then selecting the minimum one.
\par \textbf{Top-Down} Here, we work down the tree of the logical query plan from the root. For each possible implementation of the operation at the root, we consider each possible way to evaluate its argument(s) , and compute the cost of each combination, taking the best.
\par \textbf{Bottom-up} For each subexpression of the logical-query-plan tree, we com­pute the costs of all possible ways to compute that subexpression. The possibilities and costs for a subexpression E are computed by consider­ing the options for the subexpressions for E , and combining them in all possible ways with implementations for the root operator of E.
\par There is actually not much difference between the two approaches in their broadest interpretations, since either way, all possible combinations of ways to implement each operator in the query tree are considered. When limiting the search, a top-down approach may allow us to eliminate certain options that could not be eliminated bottom-up. However, bottom-up strategies that limit choices effectively have also been developed. there is an apparent simplification of the bottom-up method, where we consider only the best plan for each subexpression when we compute the plans for a larger subexpression. This approach, called \textbf{dynamic programming}, is not guaranteed to yield the best plan, although often it does. The approach called Selinger-style (or System-R-style) optimization exploits additional properties that some of the plans for a subexpression may have, in order to produce optimal overall plans from plans that are not optimal for certain subexpressions.

\subsection{Heuristic Selection}
One option is to use the same approach to selecting a physical plan that is
generally used for selecting a logical plan: make a sequence of choices based
on heuristics. Few common ones are:-
\begin{itemize}
    \item If the logical plan calls for a selection  $\sigma_{A=c}(R)$, and stored relation R has an index on attribute A, then perform an index-scan to obtain only the tuples of $R$ with $A-$value equal to $c$.
    \item if the selection involves one condition like A = c above, and other conditions as well, we can implement the selection by an index­ scan followed by a further selection on the tuples, which we shall represent by the physical operator filter.
    \item If an argument of a join has an index on the join attribute(s), then use an index-join with that relation in the inner loop.
    \item If one argument of a join is sorted on the join attribute(s) , then prefer a sort-join to a hash-join, although not necessarily to an index-join if one is possible.
    \item When computing the union or intersection of three or more relations, group the smallest relations first.
\end{itemize}

\subsection{Branch-and-Bound}
This approach, often used in practice, begins by using heuristics to find a good physical plan for the entire logical query plan. Let the cost of this plan be $C$. Then as we consider other plans for subqueries, we can eliminate any plan for a sub query that has a cost greater than $C$, since that plan for the sub query could not possibly participate in a plan for the complete query that is better than what we already know. Likewise, if we construct a plan for the complete query that has cost less than $C$, we replace $C$ by the cost of this better plan in subsequent exploration of the space of physical query plans. An important advantage of this approach is that we can choose when to cut off the search and take the best plan found so far. For instance, if the cost $C$ is small, then even if there are much better plans to be found, the time spent finding them may exceed $C$, so it does not make sense to continue the search. However, if $C$ is large, then investing time in the hope of finding a faster plan is wise.

\subsection{Hill Climbing}
This approach, in which we really search for a "valley" in the space of physical plans and their costs, starts with a heuristically selected physical plan. We can then make small changes to the plan, e.g., replacing one method for an operator by another, or reordering joins by using the associative and/or commutativen laws, to find "nearby" plans that have lower cost. When we find a plan such that no small modification yields a plan of lower cost, we make that plan our chosen physical query plan .

\subsection{Selinger-Style Optimization}
This approach improves upon the dynamic-programming approach by keeping for each subexpression not only the plan of least cost, but certain other plans that have higher cost, yet produce a result that is sorted in an order that may be useful higher up in the expression tree. Examples of such interesting orders are when the result of the subexpression is sorted on one of:
\begin{itemize}
    \item The attribute(s) specified in a sort operator $(\tau)$ at the root.
    \item The grouping attribute(s) of a later group-by operator $(\gamma)$.
    \item The join attribute(s) of a later join. 
\end{itemize}
If we take the cost of a plan to be the sum of the sizes of the intermediate relations, then there appears to be no advantage to having an argument sorted. However, if we use the more accurate measure, disk I/O's, as the cost, then the advantage of having an argument sorted becomes clear if we can use one of the sort-based algorithms and save the work of the first pass for the argument that is sorted already.

\section{Join Order}
Join takes in two argument, while the end result is independent of the order of the two arguments, the method used to compute the result maybe dependent on the order. Perhaps most important, the one-pass join reads one relation preferably the smaller
into main memory, creating a structure such as a hash table to facilitate matching of tuples from the other relation. It then reads the other relation, one block at a time, to join its tuples with the tuples stored in memory. 
\par For instance, suppose that when we select a physical plan we decide to use a one-pass join. Then we shall assume the left argument of the join is the smaller relation and store it in a main-memory data structure. This relation is called the build relation. The right argument of the join, called the probe relation, is read a block at a time and its tuples are matched in main memory with those of the build relation. Other join algorithms that distinguish between their arguments include:
\begin{itemize}
    \item Nested-loop join, where we assume the left argument is the relation of the
outer loop.
    \item Index-join, where we assume the right argument has the index.
\end{itemize}

\subsection{Join Trees}
When we have the join of two relations, we need to order the arguments. We shall conventionally select the one whose estimated size is the smaller as the left argument. Notice that the algorithms mentioned above -- one-pass, nested­ loop, and indexed - each work best if the left argument is the smaller. More precisely, one-pass and nested-loop joins each assign a special role to the smaller relation (build relation, or outer loop), and index-joins typically are reasonable choices only if one relation is small and the other has an index. It is quite common for there to be a significant and discernible difference in the sizes of arguments, because a query involving joins very often also involves a selection on at least one attribute, and that selection reduces the estimated size of one of the relations greatly.
\par When we need to join more than $2$ relations, the order in which they are joined can be represented by a binary tree, where each node has either $0$ or $2$ children. A tree where the right child always a leaf node is called a left deep tree, one can similarly define a right deep tree. Any other tree is will be called \textbf{bushy}. We will stick with left deep tree due to their interaction with various common join algorithms. This introduced limitation also helps to reduce search space.
\par If one-pass joins are used, and the build relation is on the left, then the amount of memory needed at any one time tends to be smaller than if we used a right-deep tree or a bushy tree for the same relations.
\par If we use nested-loop joins, with the relation of the outer loop on the left, then we avoid constructing any intermediate relation more than once.

\subsection{DP to decide join order}
Suppose we need to calculate $R_1 \bowtie R_2 \bowtie ... \bowtie R_n$. For the DP algorithm construct a table with an entry for each subset of one or more of the n relations. In that table we put 
\begin{itemize}
    \item the estimated size of the join of these relations.
    \item the least cost of computing the join of these relations. Other, more complex estimates, such as total disk I/O's, could be used if we were willing and able to do the extra calculation involved.
    \item The expression that yields the least cost. This expression joins the set of relations in question, with some grouping. We can optionally restrict ourselves to left-deep expressions, in which case the expression is just an ordering of the relations.
\end{itemize}

\subsection{Greedy algorithm for join order}
Even the carefully limited search of dynamic pro­gramming leads to a number of calculations that is exponential in the number
of relations joined. It is reasonable to use an exhaustive method like dynamic programming or branch-and-bound search to find optimal join orders of five or six relations. However, when the number of joins grows beyond that, or if we choose not to invest the time necessary for an exhaustive search, then we can use a join-order heuristic in our query optimizer.
\par The most common choice of heuristic is a greedy algorithm, where we make one decision at a time about the order of joins and never backtrack or reconsider decisions once made. We shall consider a greedy algorithm that only selects a left-deep tree. The "greediness" is based on the idea that we want to keep the intermediate relations as small as possible at each level of the tree.
\par Start with the pair of relations whose estimated join size is smallest. The join of these relations becomes the current tree. Find, among all those relations not yet included in the current tree, the relation that, when joined with the current tree, yields the relation of smallest estimated size. The new current tree has the old current tree as its left argument and the selected relation as its right argument.


\section{Physical Query Plan}
We have parsed the query, converted it to an initial logical query plan, and improved that logical query plan with transformations. Part of the process of selecting the physical query plan is enumeration and cost­ estimation for all of our options, then focused on the question of enumeration, cost estimation, and ordering for joins of several relations. By extension, we can use similar techniques to order groups of unions, intersections, or any associative/commutative operation
\par There are still several steps needed to turn the logical plan into a complete
physical query plan.
\begin{itemize}
    \item Selection of algorithms to implement the operations of the query plan,
when algorithm-selection was not done as part of some earlier step such
as selection of a join order by dynamic programming.
    \item Decisions regarding when intermediate results will be materialized ( cre­
ated whole and stored on disk) , and when they will be pipelined (created
only in main memory, and not necessarily kept in their entirety at any
one time) .
    \item Notation for physical-query-plan operators, which must include details
regarding access methods for stored relations and algorithms for imple­
mentation of relational-algebra operators.
\end{itemize}

\subsection{Choosing a Selection Method}
One of the important steps in choosing a physical query plan is to pick algo­rithms for each selection operator.
\par Assuming there are no multidimensional indexes on several of the attributes,
then each physical plan uses some number of attributes that each have an index, and are compared to a constant in one of the terms of the selection. We then use these indexes to identify the sets of tuples that satisfy each of the
conditions. For simplicity, we shall not consider the use of several indexes in this way. Rather, we limit our discussion to physical plans that:
\begin{itemize}
    \item Use one comparison of the form $A\theta c$, where $A$ is an attribute with an index, $c$ is a constant, and $\theta$ s a comparison operator such as $=$ or $<$.
    \item Retrieve all tuples that satisfy the comparison, using the index­ scan physical operator.
    \item Consider each tuple selected to decide whether it satisfies the rest of the selection condition. We shall call the physical operator that per­forms this step Filter; it takes the condition used to select tuples as a parameter, much as the rr operator of relational algebra does.
\end{itemize}
In addition to physical plans of this form, we must also consider the plan that uses no index but reads the entire relation (using the table-scan physical oper­ator) and passes each tuple to the Filter operator to check for satisfaction of the selection condition. 
\par We decide among the physical plans with which to implement a given selec­tion by estimating the cost of reading data for each possible option. To compare costs of alternative plans we cannot continue using the simplified cost estimate of intermediate-relation size. The reason is that we are now considering imple­mentations of a single step of the logical query plan, and intermediate relations are independent of implementation. Thus, we shall refocus our attention and resume counting disk I/O's.

\subsection{Choosing a Join Method}
On the assumption that we know (or can estimate) how many buffers are available to perform the join, we can apply the formulas for sort/ indexed/ hash join. However, if we are not sure of, or cannot know, the number of buffers that will be available during the execution of this query (because we do not know what else the DBMS is doing at the same time), or if we do not have estimates of important size parameters such as the V (R, a)'s, then there are still some principles we can apply to choosing a join method. Similar ideas apply to other binary operations such as unions, and to the full-relation, unary operators, $\gamma, \delta$


\subsection{Pipelining Versus Materialization}
The last major issue we shall discuss in connection with choice of a physical query plan is pipelining of results. The naive way to execute a query plan is to order the operations appropriately (so an operation is not performed until the argument(s) below it have been performed), and store the result of each operation on disk until it is needed by another operation. This strategy is called materialization, since each intermediate relation is materialized on disk. A more subtle, and generally more efficient, way to execute a query plan is to interleave the execution of several operations. The tuples produced by one operation are passed directly to the operation that uses it, without ever storing the intermediate tuples on disk. This approach is called pipelining, and it typically is implemented by a network of iterators whose functions call each other at appropriate times. Since it saves disk I/O 's, there is an obvious advantage to pipelining, but there is a corresponding disadvan­tage. Since several operations must share main memory at any time, there is a chance that algorithms with higher disk-I/O requirements must be chosen , or thrashing will occur, thus giving back all the disk-I/O savings that were gained by pipelining, and possibly more.


\subsection{Pipelining Unary Operations}
Unary operations - selection and projection - are excellent candidates for pipelining. Since these operations are tuple-at-a-time, we never need to have more than one block for input, and one block for the output. We may implement a pipelined unary operation by iterators, The consumer of the pipelined result calls \textbf{GetNext()} each time another tuple is needed. In the case of a projection, it is only necessary to call \textbf{GetNext()} once on the source of tuples, project that tuple appropriately, and return the result to the consumer. For a selection $\sigma_C$ (technically, the physical operator \textbf{Filter(C)} ) , it may be necessary to call \textbf{GetNext()} several times at the source, until one tuple that satisfies condition $c$ is found.


\subsection{Pipelining Binary Operations}
The results of binary operations can also be pipelined. We use one buffer to pass the result to its consumer, one block at a time. However, the number of other buffers needed to compute the result and to consume the result varies, depending on the size of the result and the sizes of other relations involved in the query. We shall use an extended example to illustrate the tradeoffs and opportunities.


\subsection{Notation for Physical Query Plans}

\par \textbf{Operators for Leaves}
\par Each relation R that is a leaf operand of the logical-query-plan tree will be replaced by a scan operator. The options are:
\begin{itemize}
    \item \textbf{TableScan(R):} All blocks holding tuples of R are read in arbitrary order.
    \item \textbf{SortScan(R,L):} Tuples of R are read in order, sorted according to the attribute(s) on list L.
    \item \textbf{IndexScan(R,C):} Here, C i s a condition of the form $A\theta c$, where A is an attribute of $R, \theta$ is a comparison such as $=$ or $<$ , and $c$ is a constant. Tu­ples of $R$ are accessed through an index on attribute $A$. If the comparison $\theta$ is not $=$, then the index must be one, such as a B-tree, that supports range queries.
    \item \textbf{IndexScan(R,A):} Here $A$ is an attribute of $R$. The entire relation $R$ is retrieved via an index on $R.A.$ This operator behaves like \textbf{TableScan}, but may be more efficient in certain circumstances, if $R$ is not clustered and/or its blocks are not easily found.
\end{itemize}

\par \textbf{Physical Operators for Selection}
\par A logical operator $\sigma_C(R)$ is often combined, or partially combined, with the access method for relation $R$, when $R$ is a stored relation. Other selections, where the argument is not a stored relation or an appropriate index is not available, will be replaced by the corresponding physical operator we have called \textbf{Filter}.

\par \textbf{Physical Sort Operators}
\par Sorting of a relation can occur at any point in the physical query plan. We have already introduced the \textbf{SortScan(R, L)} operator, which reads a stored relation $R$ and produces it sorted according to the list of attributes $L$. When we apply a sort-based algorithm for operations such as join or grouping, there is an initial phase in which we sort the argument according to some list of attributes. It is common to use an explicit physical operator $Sort(L)$ to perform this sort on an operand relation that is not stored. This operator can also be used at the top of the physical-query-plan tree if the result needs to be sorted because of an \textbf{ORDER BY} clause in the original query,

\par \textbf{Other Relational-Algebra Operations}
\par All other operations are replaced by a suitable physical operator. These oper­ators can be given designations that indicate:
\begin{itemize}
    \item The operation being performed, e.g., join or grouping.
    \item Necessary parameters, e.g., the condition in a theta-join or the list of elements in a grouping.
    \item A general strategy for the algorithm: sort-based, hash-based, or in some joins, index-based.
    \item A decision about the number of passes to be used: one-pass, two-pass, or multipass (recursive, using as many passes as necessary for the data at hand) . Alternatively, this choice may be left until run-time.
    \item An anticipated number of buffers the operation will require.
\end{itemize}
\subsection{Ordering of Physical Operations}
Our final topic regarding physical query plans is the matter of order of oper­ations. The physical query plan is generally represented as a tree, and trees imply something about order of operations, since data must flow up the tree.However, since bushy trees may have interior nodes that are neither ancestors nor descendants of one another, the order of evaluation of interior nodes may not always be clear. Moreover, since iterators can be used to implement opera­tions in a pipelined manner, it is possible that the times of execution for various nodes overlap, and the notion of "ordering" nodes makes no sense. 
\par If materialization is implemented in the obvious store-and-later-retrieve way, and pipelining is implemented by iterators, then we may establish a fixed se­quence of events whereby each operation of a physical query plan is executed.The following rules summarize the ordering of events i m pl icit in a phy sical query-plan tree:
\begin{itemize}
    \item Break the tree into subtrees at each edge that represents materialization. The subtrees will be executed one-at-a-time.
    \item Order the execution of the subtrees in a bottom-up, left-to-right manner. To be precise, perform a preorder traversal of the entire tree. Order the subtrees in the order in which the preorder traversal exits from the subtrees.
    \item Execute all nodes of each subtree using a network of iterators. Thus, all the nodes in one subtree are executed simultaneously, with GetNext calls among their operators determining the exact order of events.
\end{itemize}

\section{Introduction to Data Streams}
The sequence of data items continuously generated by sources is termed a data stream. Because of the possible never-ending nature of a data stream, the amount of data to be processed is likely to be unbounded. In addition, timely detection of interesting changes or patterns or aggregations over incoming data is critical for many of these applications. Furthermore, the data arrival rates may fluctuate over a period of time and may be bursty at times. For most of these applications, Quality of Service (or QoS) requirements, such as response time, memory usage, and throughput are extremely important. These application requirements make it infeasible to simply load the incoming data streams into a persistent store and process them effectively using currently available database management techniques.

\section{Data stream windowing}


\section{Query Processing of data streams(Combine the DBMS and DSMS)}


\section{Challenges of query optimization on data streams}


\section{Conclusion and discussion}



\begin{comment}

\section{System R}
\label{sec:systemR}
\section{Deep Reinforcement learning}
\label{sec:drl}

Markov decision process(MDP) is used to formalize various types of stochastic processes. In MDPs, the goal of the agent is to make a sequence of actions to optimize/ maximize an objective function. \\
Formally a MDP is a $5-$tuple 
\begin{center}
$\langle S,A,P(s,a),R(s,a), s_0 \rangle $\\
$S \to $ Set of all possible states the agent can be in.\\
$A \to $ Set of all possible actions the agent can take.\\
$P(s,a) \to $ A probability distribution of going to various states given current state and action. $s^{1} \sim P(s,a)$\\
$R(s,a) \to $ Reward for taking action $a$ on state $s$.\\
$s_0 \to $ Describes the initial state of the system/ agent.
\end{center}
The performance of the agent is measured using the rewards collected along the way through various states. So the objective of an MDP is to find a policy $\pi:S\rightarrow A$, a function that maps states to actions, in order to maximize the expected value:-
\begin{center}
    \[ 
    \argmax_{\pi} \mathbb{E} \left[ \sum^{T-1}_{t=0}R(s_{t},a_{t})  \right]
    \] 
    subject to $s_{t+1} = P(s_{t},a_{t}), a_{t} = \pi{s_{t}}$
\end{center}
This method does not reduce the search space, and unlikely greedy solution, this will lead to an optimal solution. This method does not reduce the search space, and unlikely greedy solution, this will lead to an optimal solution.
\par Reinforcement learning(RL) is a technique which optimizes MDPs iteratively, by running a simulation in each iteration and changing the policy to find an optimal one based on the cumulative reward.
\section{Relations}
\label{sec:relations}
A common method/ data structure used to formalize joins
\par \textbf{Query Graph $\to$} A query graph $G$ is an undirected graph, where each relation R is a vertex and each join predicate $\rho$ defines an edge between $2$ vertices. Let $\kappa_G$ denote the number of connected components in $G$
\par A join of relation $R_1, R_2$, in the graph corresponds to remove the vertices $v_{R_1}, v_{R_2}$, replacing them with a vertex $v_{R_1+R_2}$, the edges of the form $(v_{R_1},v) \& (v_{R_2},v)$ are replaced by $(v_{R_1+R_2}, v)$. Note each reduction reduces number of vertices by one, so this process is repeated until there are $\kappa_G$ number of vertices left.
\par \textbf{Join Optimization Problem $\to$} Let $G$ be a query graph and $J$ be a join cost model. Find sequence, $c_1\circ c_2\circ ... \circ c_n$ resulting in $|V| = \kappa_G$ to minimize
\begin{center}
    \[
        \min_{c_1,c_2,...c_n} \sum^{n}_{i=1}J(c_i)
    \] 
    subject to $G_{i+1} = c(G_i)$
\end{center}
Using these definitions, we define a MDP. 
\begin{center}
$\langle \{G_0,G_1,...G_T\},c,P(G,c),-J, G \rangle $\\
\end{center}
We are still not certain about how the cost function $J$ is structured. We are still not certain about how the cost function $J$ is structured. We are still not certain about how the cost function $J$ is structured.

We are still not certain about how the cost function $J$ is structured.

\end{comment}







